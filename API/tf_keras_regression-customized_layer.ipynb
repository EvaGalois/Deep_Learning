{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100, input_shape=[None, 5])\n",
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 0.2133239 , -0.19240446,  0.19820555,  0.20316713,  0.10902579,\n",
       "         -0.23679228,  0.07624881,  0.185335  ,  0.22776116, -0.23059395,\n",
       "         -0.23103987,  0.00399624, -0.22992799, -0.1199474 ,  0.1200106 ,\n",
       "         -0.165461  , -0.00381915,  0.06670602, -0.02617913,  0.00659665,\n",
       "          0.05440591,  0.23702584,  0.22699873, -0.13313682,  0.22823982,\n",
       "          0.1258363 ,  0.17022447, -0.01152591, -0.23829912, -0.04966518,\n",
       "         -0.18049414,  0.06796642,  0.06761028,  0.05168141, -0.20216542,\n",
       "         -0.04308969,  0.13423987,  0.16210555,  0.07746212,  0.00952922,\n",
       "         -0.08487543, -0.21563838, -0.08766221,  0.23360832, -0.03040563,\n",
       "         -0.13138664,  0.11682557,  0.14885668, -0.01864506,  0.03523009,\n",
       "          0.1892037 ,  0.19009672,  0.23335971, -0.05275649,  0.05901141,\n",
       "         -0.2355394 ,  0.10987253,  0.16206656, -0.04343785, -0.06888172,\n",
       "          0.04360045, -0.05986613,  0.04993831, -0.00242716,  0.00221805,\n",
       "         -0.22149332, -0.11775602,  0.05848096,  0.1091962 , -0.10036434,\n",
       "          0.10735743, -0.05744666, -0.04580751, -0.17153239, -0.18341987,\n",
       "          0.23056729, -0.11271761,  0.13061039,  0.08426122, -0.15692852,\n",
       "         -0.01712307,  0.08897226,  0.14619105, -0.20897374, -0.22664571,\n",
       "         -0.21199214, -0.08976007,  0.12540106,  0.15629874, -0.21160078,\n",
       "          0.12848659, -0.04203908, -0.10915032,  0.13528068, -0.218516  ,\n",
       "          0.0447226 , -0.08265224, -0.13755435, -0.16864884,  0.13106553],\n",
       "        [ 0.0928985 ,  0.17998649,  0.1359068 , -0.23865795, -0.19013911,\n",
       "          0.21993525,  0.0253884 , -0.00400871,  0.0274062 , -0.23708147,\n",
       "         -0.04645751,  0.1258011 ,  0.12488042,  0.01764895, -0.02351113,\n",
       "         -0.04125503, -0.22018972, -0.08068582,  0.20795955, -0.23894587,\n",
       "          0.22223906,  0.2011693 , -0.02538927,  0.12328692, -0.10768931,\n",
       "         -0.22287431, -0.11911941,  0.20172076,  0.18573265,  0.00778376,\n",
       "          0.20148037, -0.05647117, -0.07030968, -0.15409935, -0.16361944,\n",
       "          0.08608778,  0.07863112,  0.12785785,  0.18524964, -0.08554196,\n",
       "          0.01698218, -0.00333425, -0.17259538, -0.19167212,  0.19865744,\n",
       "         -0.21645224,  0.02914111, -0.05472252,  0.05247556,  0.16312857,\n",
       "         -0.09552079, -0.10776784, -0.00107346,  0.11160575, -0.2052187 ,\n",
       "         -0.05524218,  0.18472974, -0.12469315, -0.2230763 ,  0.06754689,\n",
       "          0.07834516, -0.22141945, -0.21574992, -0.13219632, -0.12032133,\n",
       "          0.23798142,  0.18655227,  0.01492034, -0.14412734, -0.13673154,\n",
       "          0.03687985,  0.21503557, -0.00417428,  0.06442101, -0.22732666,\n",
       "          0.14179273, -0.23415881,  0.03127392,  0.08377625, -0.06110345,\n",
       "         -0.05403376, -0.20427679, -0.01416354, -0.16962233,  0.07167278,\n",
       "         -0.15669042, -0.16417222,  0.06787239,  0.1975184 ,  0.06659813,\n",
       "          0.06106685, -0.21755522, -0.2001453 , -0.02286984,  0.20816605,\n",
       "          0.20384826, -0.23466787,  0.049814  ,  0.17752703,  0.21081899],\n",
       "        [ 0.00850163,  0.20893623, -0.1425882 , -0.07832745, -0.09806508,\n",
       "         -0.20677945,  0.07833664,  0.03800283, -0.04062434,  0.08595739,\n",
       "          0.18947048,  0.14540426,  0.12755327, -0.03282012, -0.23104876,\n",
       "          0.12613489,  0.11225758, -0.0654165 ,  0.00573936,  0.1560378 ,\n",
       "         -0.05044296,  0.07888307, -0.15536731, -0.06009227, -0.23788421,\n",
       "          0.23783912,  0.03089951,  0.14630942,  0.06972267, -0.04846372,\n",
       "          0.14885919, -0.11101632,  0.10519831, -0.03724051, -0.15708047,\n",
       "          0.2343838 ,  0.1776978 , -0.03308737, -0.20384455, -0.13306683,\n",
       "         -0.02353096, -0.1829229 ,  0.23820634,  0.02227576,  0.01752453,\n",
       "          0.14375924, -0.02134995,  0.03419383, -0.2332186 , -0.01751684,\n",
       "          0.06751908,  0.20817505, -0.09805071, -0.05198264,  0.1876541 ,\n",
       "          0.03923486,  0.07736655,  0.03937392,  0.2044261 ,  0.0424007 ,\n",
       "         -0.18866242, -0.16612461,  0.06474034, -0.09345584, -0.00435694,\n",
       "         -0.06201226,  0.01018219, -0.20535292,  0.016717  , -0.0904161 ,\n",
       "         -0.12100861, -0.182808  ,  0.2189777 , -0.14264193,  0.18521275,\n",
       "          0.20490985, -0.21085827, -0.14602862, -0.151645  ,  0.13012479,\n",
       "         -0.23517813, -0.05648731,  0.21395259,  0.0077242 , -0.18581398,\n",
       "          0.16363968,  0.03553356, -0.10816708,  0.03878005,  0.08030738,\n",
       "          0.07821591,  0.13483106, -0.17791304,  0.197836  ,  0.02715863,\n",
       "         -0.17261925, -0.0407235 , -0.20032899,  0.22793274, -0.15222383],\n",
       "        [ 0.13385682, -0.19840845,  0.1401278 ,  0.05015902, -0.07902573,\n",
       "          0.13485341, -0.08795966, -0.17024219, -0.22226113,  0.19503145,\n",
       "         -0.2334485 , -0.10720441,  0.23814614,  0.14462005,  0.09091555,\n",
       "          0.10488977, -0.19583407,  0.1980743 , -0.04197302,  0.09468795,\n",
       "         -0.05302823, -0.06834719, -0.18734674, -0.17563093, -0.04976887,\n",
       "         -0.039333  , -0.2212755 , -0.08512746,  0.18563144, -0.17319962,\n",
       "         -0.00675692, -0.01236622, -0.11208101, -0.16862759, -0.09309039,\n",
       "          0.09234725,  0.08330403, -0.19622904,  0.08514713, -0.18511154,\n",
       "         -0.12228394,  0.08015464, -0.20433366,  0.16465695,  0.18108623,\n",
       "          0.05570079, -0.05091459, -0.19132537, -0.13166595,  0.05580698,\n",
       "         -0.03031386, -0.21950307,  0.06191264,  0.11963736,  0.2016216 ,\n",
       "         -0.12328587,  0.19826533,  0.13205214, -0.12301175, -0.22043006,\n",
       "          0.09179021,  0.18503515, -0.19500454, -0.02723739, -0.08202806,\n",
       "         -0.05078259,  0.06712122,  0.22733338, -0.11514917, -0.14388534,\n",
       "         -0.04407167, -0.21105558, -0.03109221, -0.14657319, -0.04806864,\n",
       "         -0.21807858,  0.01314239, -0.10311989, -0.1526271 , -0.09367645,\n",
       "         -0.21256435, -0.01780356, -0.0073261 ,  0.23721676, -0.01143324,\n",
       "          0.15298964,  0.00165007, -0.21749458,  0.0053591 ,  0.14765371,\n",
       "         -0.18716665,  0.10801159,  0.18475772,  0.02801876,  0.18025453,\n",
       "          0.06220801, -0.1899047 ,  0.19587676,  0.18258493, -0.1580657 ],\n",
       "        [ 0.1352794 ,  0.1052096 , -0.07712404, -0.01699901,  0.16062559,\n",
       "          0.22861601,  0.04018413,  0.04573239, -0.02364375, -0.18311371,\n",
       "          0.23617984, -0.01730426, -0.1596047 ,  0.00355625,  0.02878149,\n",
       "          0.2147681 , -0.10659122, -0.19607088,  0.04033811, -0.0170801 ,\n",
       "         -0.20078409, -0.21416198, -0.1814388 ,  0.12105359, -0.13748488,\n",
       "         -0.10852225, -0.18753198,  0.22711603, -0.22550945,  0.13619296,\n",
       "          0.22190852,  0.14857529,  0.14954202,  0.2320251 ,  0.05619229,\n",
       "          0.1419598 ,  0.16925691, -0.07671489, -0.02861735,  0.15873076,\n",
       "         -0.18595195, -0.05454543, -0.21352834,  0.13998385, -0.13838467,\n",
       "          0.04634582, -0.17747049,  0.10946582, -0.07980186, -0.09051073,\n",
       "          0.18206681,  0.14840998,  0.07881658,  0.18359046,  0.13708358,\n",
       "          0.05761473, -0.00335637, -0.10440469, -0.17724001, -0.00593382,\n",
       "         -0.2379889 , -0.00947644,  0.10065337,  0.13868691, -0.22162709,\n",
       "          0.06303595,  0.11264785, -0.1012242 , -0.23015688,  0.19870497,\n",
       "          0.08747955,  0.22221275,  0.14324208,  0.22518726,  0.07977916,\n",
       "         -0.16735277,  0.20141785, -0.04528351,  0.1511405 , -0.05815965,\n",
       "          0.07030155, -0.00712179,  0.08112369,  0.07437824,  0.116495  ,\n",
       "          0.19499855, -0.23869544, -0.07352203, -0.04453109, -0.20614278,\n",
       "          0.12476446,  0.14411126,  0.07088144, -0.2129657 , -0.1299536 ,\n",
       "          0.23882471,  0.07702859,  0.1653205 ,  0.14563297, -0.10769375]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer.variables\n",
    "# x * w + b\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then\n",
      " |  it is flattened prior to the initial dot product with `kernel`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # as first layer in a sequential model:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(16,)))\n",
      " |  # now the model will take as input arrays of shape (*, 16)\n",
      " |  # and output arrays of shape (*, 32)\n",
      " |  \n",
      " |  # after the first layer, you don't need to specify\n",
      " |  # the size of the input anymore:\n",
      " |  model.add(Dense(32))\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\")..\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California housing dataset.\n",
      "\n",
      "The original database is available from StatLib\n",
      "\n",
      "    http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The data contains 20,640 observations on 9 variables.\n",
      "\n",
      "This dataset contains the average house value as target variable\n",
      "and the following input variables (features): average income,\n",
      "housing average age, average rooms, average bedrooms, population,\n",
      "average occupation, latitude, and longitude in that order.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "Statistics and Probability Letters, 33 (1997) 291-297.\n",
      "\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.fit_transform(x_valid)\n",
    "x_test_scaled = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softplus: log(1+e^x)\n",
    "customized_softplus = keras.layers.Lambda(lambda x : tf.nn.softplus(x))\n",
    "print(customized_softplus([-10., -5., 0., 5., 10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_4 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_5 (Cu (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# customized dense layer.\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\"\"\"\n",
    "        # x * w + b. input_shape:[None, a] [a, b] output_shape:[None, b]\n",
    "        self.kernel = self.add_weight(name = 'kernel',\n",
    "                                     shape = (input_shape[1], self.units),\n",
    "                                     initializer = 'uniform',\n",
    "                                     trainable = True)\n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                   shape = (self.units,),\n",
    "                                   initializer = 'zeros',\n",
    "                                   trainable = True)\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu', input_shape=x_train.shape[1:]),\n",
    "    CustomizedDenseLayer(1),\n",
    "    customized_softplus,\n",
    "    # keras.layers.Dense(1, activation=\"softplus\")\n",
    "    # keras.layers.Dense(1), keras.layers.Activation('softplus'),\n",
    "    \n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 1.2250 - val_loss: 0.6734\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5942 - val_loss: 0.5790\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5227 - val_loss: 0.5078\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4810 - val_loss: 0.4743\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4617 - val_loss: 0.4979\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4656 - val_loss: 0.4418\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4392 - val_loss: 0.4397\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4319 - val_loss: 0.4332\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4402 - val_loss: 0.4298\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4195 - val_loss: 0.4176\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4106 - val_loss: 0.4149\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4079 - val_loss: 0.4117\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4068 - val_loss: 0.4310\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4056 - val_loss: 0.4194\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3888 - val_loss: 0.4090\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, validation_data=(x_valid_scaled, y_valid), epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 18us/sample - loss: 0.4292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4292053958242254"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
