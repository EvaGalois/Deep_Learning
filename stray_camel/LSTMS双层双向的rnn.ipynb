{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36764bitrecenvvirtualenvc590a763a4564b6f98e8e2c90348aa96",
   "display_name": "Python 3.6.7 64-bit ('rec_env': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN Recurrent Neural Network\n",
    "卷曲神经网络\n",
    "- Embedding 与变长输入的处理\n",
    "- 序列式问题\n",
    "- 循环神经网络\n",
    "- LSTM模型,拥有更好的记忆能力\n",
    "\n",
    "长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 为什么需要循环神经网络\n",
    "合并＋padding的缺点\n",
    "- 信息丢失\n",
    "    - 多个embedding合并，pad变成噪音，就算没有许多pad，也会导致数据无主次\n",
    "    - 有些无关词语过多会导致数据被稀释掉\n",
    "- 无效计算太多，抵消，太多的pad噪音，和次要词都会进行计算，务必要 "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 序列式问题\n",
    "- 普通神经网络：`x-rnn-y`\n",
    "- 图片生成描述：`x-rnn-[y1,y2,...]`\n",
    "- 文本分类（文本情感分析）：`[x1,x2,...]-rnn-y`\n",
    "- 多对多，encoding-decoding 机器翻译：`[x1,x2,...]-rnn-[y1,y2,...]`"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 循环神经网络\n",
    "- 维护一个状态作为下一步的额外输入\n",
    "- 每一步使用同样的激活函数和参数\n",
    "- 每一步不仅得到一个输出，还得到一个状态数值\n",
    "\n",
    "公式,最简单的循环神经网络：\n",
    "$$s_t = f_w(s_{t-1},x_t) ,s_t 新状态 s_{t-1} 为旧状态 $$\n",
    "将以上公式展开：\n",
    "$$s_t = tanh(W_{s_{t-1}},u_{x_t}),W_{s_{t-1}},u_{x_t},分别是对s_{t-1},x_t数据的矩阵变换$$\n",
    "$$\\hat{y_t} = softmax(V_{s_t}),相当于对s_t做一个全连接层$$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 循环神经网络做文本分类\n",
    "```\n",
    "x1 - rnn1\n",
    "x2 - rnn2\n",
    "x3 - rnn3   ===合并==mlp==y\n",
    "x4 - rnn4\n",
    "x5 - rnn5\n",
    "\n",
    "奇中x1-x5输入到embedding层的rnn层后会产生一个和上下rnn层相互作用的状态值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "import os, sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据集imdb  \n",
    "imdb = keras.datasets.imdb\n",
    "# 设定词表的个数，统计所有的词语并将前一万个保留下来\n",
    "vocab_size = 10000\n",
    "# 词表的index从3开始算\n",
    "index_from =3\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, index_from=index_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0], train_labels[0])\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(len(train_data[0]), len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "print(len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整体的序列向右移三位，如下添加四个自定义的字符\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding填充的字符\n",
    "word_index['<PAD>'] = 0 \n",
    "# 每个字符开始的字符\n",
    "word_index['<START>'] = 1\n",
    "# 不知道的字符设置\n",
    "word_index['<UNK>'] = 2\n",
    "# 每个字符结束的字符\n",
    "word_index['<END>'] = 3\n",
    "\n",
    "# 将word_index 反转成index：word\n",
    "reverse_word_index = {v:k for k, v in word_index.items()}\n",
    "\n",
    "decode_review = lambda test_ids:''.join([reverse_word_index.get(word_id, '<UNK>') for word_id in test_ids])\n",
    "\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_data, # list of list\n",
    "    value=word_index['<PAD>'],\n",
    "    padding='post', # post放在补全的后面，pre放在补全的前面\n",
    "    maxlen = max_length,\n",
    ")\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_data, # list of list\n",
    "    value=word_index['<PAD>'],\n",
    "    padding='post', # post放在补全的后面，pre放在补全的前面\n",
    "    maxlen = max_length,\n",
    ")\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个embedding定为长度为16的向量\n",
    "max_length = 500\n",
    "embedding_dim = 16\n",
    "batch_size = 128\n",
    "# 实现一个单层单向的LSTM\n",
    "bi_rnn_model = keras.models.Sequential([\n",
    "    # 1. define matrix : [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    # 添加一个rnn层，先定义一个简单的rnn网络层，return_sequences= False表示我们只是用最后的输出\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=False)),\n",
    "    # 全连接层\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "\n",
    "])\n",
    "bi_rnn_model.summary()\n",
    "bi_rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2作为验证集\n",
    "bi_rnn_history = bi_rnn_model.fit(train_data, train_labels, epochs=30, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data['val_'+label] = history.history['val_'+label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(single_rnn_history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(single_rnn_history, 'loss', 30, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_rnn_model.evaluate(test_data, test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像分析\n",
    "过拟合现象\n",
    "对valid数据loss比较小，但是对于预测数据loss过于大\n",
    "解决办法：\n",
    "过拟合解决方法：\n",
    "\n",
    "1. 增加训练数据，数据量变多\n",
    "2. L1、L2...regularization\n",
    "    - y=Wx\n",
    "    - l1 : cost = (Wx-realy)^2 + abs(W)\n",
    "    - l2 : cost = (Wx-realy)^2 + sqrt(W)\n",
    "    - l3 : cost = (Wx-realy)^2 + pow(W)\n",
    "3. 简化神经网络结构\n",
    "    - droput regularization\n",
    "\n",
    "跑一次训练集要好久，顶不住，方法mark了以后加入到自己的项目做分析。。"
   ]
  }
 ]
}