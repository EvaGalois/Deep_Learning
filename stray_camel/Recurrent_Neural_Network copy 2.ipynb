{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36764bitrecenvvirtualenvc590a763a4564b6f98e8e2c90348aa96",
   "display_name": "Python 3.6.7 64-bit ('rec_env': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN Recurrent Neural Network\n",
    "卷曲神经网络\n",
    "- Embedding 与变长输入的处理\n",
    "- 序列式问题\n",
    "- 循环神经网络\n",
    "- LSTM模型,拥有更好的记忆能力\n",
    "\n",
    "长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 为什么需要循环神经网络\n",
    "合并＋padding的缺点\n",
    "- 信息丢失\n",
    "    - 多个embedding合并，pad变成噪音，就算没有许多pad，也会导致数据无主次\n",
    "    - 有些无关词语过多会导致数据被稀释掉\n",
    "- 无效计算太多，抵消，太多的pad噪音，和次要词都会进行计算，务必要 "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 序列式问题\n",
    "- 普通神经网络：`x-rnn-y`\n",
    "- 图片生成描述：`x-rnn-[y1,y2,...]`\n",
    "- 文本分类（文本情感分析）：`[x1,x2,...]-rnn-y`\n",
    "- 多对多，encoding-decoding 机器翻译：`[x1,x2,...]-rnn-[y1,y2,...]`"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 循环神经网络\n",
    "- 维护一个状态作为下一步的额外输入\n",
    "- 每一步使用同样的激活函数和参数\n",
    "- 每一步不仅得到一个输出，还得到一个状态数值\n",
    "\n",
    "公式,最简单的循环神经网络：\n",
    "$$s_t = f_w(s_{t-1},x_t) ,s_t 新状态 s_{t-1} 为旧状态 $$\n",
    "将以上公式展开：\n",
    "$$s_t = tanh(W_{s_{t-1}},u_{x_t}),W_{s_{t-1}},u_{x_t},分别是对s_{t-1},x_t数据的矩阵变换$$\n",
    "$$\\hat{y_t} = softmax(V_{s_t}),相当于对s_t做一个全连接层$$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 循环神经网络做文本分类\n",
    "```\n",
    "x1 - rnn1\n",
    "x2 - rnn2\n",
    "x3 - rnn3   ===合并==mlp==y\n",
    "x4 - rnn4\n",
    "x5 - rnn5\n",
    "\n",
    "奇中x1-x5输入到embedding层的rnn层后会产生一个和上下rnn层相互作用的状态值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "import os, sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据集imdb  \n",
    "imdb = keras.datasets.imdb\n",
    "# 设定词表的个数，统计所有的词语并将前一万个保留下来\n",
    "vocab_size = 10000\n",
    "# 词表的index从3开始算\n",
    "index_from =3\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, index_from=index_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n(25000,) (25000,)\n218 189\n"
    }
   ],
   "source": [
    "print(train_data[0], train_labels[0])\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(len(train_data[0]), len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(25000,) (25000,)\n"
    }
   ],
   "source": [
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ": 88151, 'gravestones': 88152, 'freshmen': 23366, 'formatted': 34646, 'drooping': 88153, 'zelig': 76033, 'yakusyo': 88154, 'lunceford': 82057, 'editorializing': 88155, 'plywood': 34647, 'banalities': 88156, 'nestor': 30536, 'revitalizes': 64826, 'voguing': 40752, 'sedate': 21666, 'dictum': 51857, 'brasher': 88157, 'york': 779, 'unchallengeable': 88158, 'subtelly': 88159, 'opposition': 8772, 'fetchingly': 88160, \"'secrets\": 70076, 'appearance\\x85': 88161, 'teleflick': 88162, 'viennese': 19481, 'orphanage': 10076, 'movers': 40753, \"cameraman's\": 27605, \"cameraman't\": 88163, 'pornoes': 88164, 'embodiments': 51858, 'heorine': 88165, 'fraternity': 16105, \"'procedures'\": 88166, 'finds': 656, 'caratherisic': 88167, 'munshi': 27606, 'clashing': 20584, \"mjh's\": 40754, 'lärm': 88168, 'nikah': 76037, 'incandescent': 51859, 'stowing': 51860, 'acrid': 51861, 'eyewitness': 25213, 'maniacally': 24001, 'suspenders': 51863, 'acupat': 57060, 'nominee': 11582, 'toshiro': 23367, \"'anita\": 51864, 'ciannelli': 25214, 'clyde': 8286, 'posher': 76039, 'johannes': 34649, 'predeccesors': 88169, 'watchword': 88170, 'change': 650, 'talkshow': 51865, 'ska': 51984, \"'colorful'\": 88171, 'suffocate': 51970, 'pathos': 6965, \"dial's\": 40755, \"'notorious'\": 88172, 'slideshow': 88173, 'americaness': 88174, '1861': 34650, '1860': 34651, '1863': 40756, '1862': 51866, '1865': 35867, '1864': 51868, 'misbehaving': 51869, \"'plague'\": 88175, 'heighten': 18495, 'toppling': 88176, 'appallonia': 88177, 'fernandel': 88178, 'tenuta': 51870, 'civility': 27607, 'mikaele': 88179, \"house's\": 18948, \"cahiil's\": 88180, 'abos': 88181, \"parents'\": 9258, 'fernandes': 88182, \"ouedraogo's\": 40757, 'boromir': 27609, 'moustache': 11922, \"boothe's\": 86719, 'fernandez': 19482, \"star's\": 17623, \"hoover's\": 34652, 'riz': 40758, 'categorized': 19483, 'gastronomic': 88183, 'flitted': 88184, 'rip': 1674, 'rin': 34421, 'rio': 16078, 'ril': 51871, 'rim': 34653, \"movin'\": 40760, 'rif': 88185, 'rig': 18496, 'rid': 3764, 'rib': 23368, 'ric': 40761, 'ethnicity': 16502, 'blackwood': 14350, \"what'\": 88186, 'ignacio': 88187, 'lengthy': 4670, 'yidische': 88188, 'eames': 88189, 'lengths': 11571, 'bacula': 88191, \"'certain\": 88192, 'ideologies': 18497, 'propping': 51872, 'chicory': 88193, 'hester': 88194, 'apeal': 88195, 'minis': 27611, 'novelizations': 88196, 'devgn': 88197, 'targetting': 88198, 'brooding': 6040, 'moving': 725, 'incapacitated': 30665, 'uneasily': 88199, 'obit': 88200, 'noodle': 14351, 'castigates': 88201, \"shame's\": 51873, 'solemnity': 34654, 'antoniette': 88202, 'limbaugh': 88203, 'abrasive': 17142, 'analysis': 4759, 'solids': 88204, 'castigated': 88205, 'broods': 34655, 'starved': 13415, 'huggie': 88206, \"'rangi\": 88207, 'silvestres': 88208, 'bankrolls': 88209, 'reincarnates': 88210, 'misguiding': 88211, 'orientalism': 88212, 'trickle': 37476, 'mysteriosity': 88213, 'bancroft': 21892, 'reincarnated': 13875, 'orientalist': 88214, 'inference': 30537, 'cabaret': 12994, \"santos's\": 88215, 'gameel': 88216, \"stubby's\": 88217, 'dabbie': 88218, 'navigator': 51874, 'thrillingly': 51875, 'frightner': 88219, \"'sudden\": 51876, 'violations': 34656, 'essanay': 51877, 'devourer': 88220, 'joyless': 19485, 'beals': 17624, 'unlimited': 16107, \"matsujun's\": 88221, 'kass': 88222, 'helfgotts': 88223, 'kasi': 51878, 'kase': 88224, \"felt'\": 76047, 'woefull': 88225, 'glittery': 88226, 'pithy': 30538, 'cameroon': 17913, 'glitters': 88227, 'incidental': 8168, 'italians': 9653, 'breeder': 34657, 'splatterfest': 51879, 'stefanovic': 88228, 'insector': 88229, '¨10': 88230, 'resourcefully': 88231, 'impassioned': 20585, 'burgundians': 30539, 'eluding': 51880, 'traits': 6790, 'marmalade': 51881, 'hunziker': 88232, 'highschoolers': 88233, \"'side\": 88234, 'ribisi': 16807, 'indictable': 88235, 'anny': 88236, 'pros': 7178, \"merry's\": 88238, 'prop': 7913, 'anno': 51882, 'prom': 3898, \"bogayevicz's\": 88239, 'necrophilia': 20586, 'prof': 30540, 'andronicus': 88240, 'prod': 25215, 'prob': 51883, 'apathetic': 16808, 'tilts': 63932, 'swatman': 88242, \"carmen'\": 51884, 'mosh': 50029, 'yamasaki': 88244, 'wooofff': 88245, 'gollam': 88246, 'weasels': 34658, 'jetson': 86472, 'jetsom': 88247, 'leonel': 88248, 'intense': 1593, \"'anastasia\": 57118, \"glenda's\": 88250, 'mungle': 88251, \"j's\": 34659, 'schaffer': 30541, 'tortuous': 18498, 'littauer': 88252, 'unimpressed': 19486, 'greets': 20579, \"'toothbrush'\": 88254, 'credible': 3081, 'cutoff': 88255, \"montage's\": 88256, 'incoherence': 23369, \"'waster'\": 88257, 'hatcheck': 88258, 'because': 85, 'piteous': 88260, 'incoherency': 34660, 'credibly': 21893, 'colorize': 51885, 'slavoj': 18436, \"brat's\": 88261, 'ioffer': 88262, 'pallio': 88263, 'sndtrk': 44879, \"steven's\": 51886, 'pallid': 40763, 'doing\\x85': 88265, 'signifies': 25133, 'mindbogglingly': 88266, 'unadorned': 51887, \"beagle's\": 88267, 'stupefaction': 88268, 'signified': 20587, \"already's\": 88269, \"'humans\": 51888, \"laughed'\": 51889, \"vita's\": 88270, 'perplexing': 13877, 'conspicious': 88271, 'skewers': 34662, 'trussed': 41764, 'embarasses': 88272, 'birthmark': 51986, 'orgue': 88273, 'crank': 13878, 'siodmak': 18499, 'familia': 88274, 'berkovits': 88275, 'embarassed': 51890, 'crane': 15471, 'billed': 5170, '115': 34663, \"grierson's\": 88276, 'soutendjik': 88277, 'astounds': 51891, 'araújo': 88278, \"'human'\": 51892, 'caller': 18455, 'bonestell': 88279, 'torpedoing': 51607, \"'plan\": 40765, \"arkin's\": 27612, \"avigdor's\": 88280, 'didn´t': 25216, 'holler': 34664, 'obsolescence': 51894, 'called': 443, '110': 16108, 'gerda': 51895, 'samaurai': 88281, 'gerde': 88282, 'soars': 27613, \"philadelphia'\": 88283, 'entrance': 7432, \"board's\": 51897, 'ether': 27614, 'actores': 88284, \"jawab'\": 88285, 'anamorphic': 14894, 'haney': 88286, 'nilsen': 88287, 'associates’': 88288, 'tentative': 19487, 'connoisseurship': 88289, 'wg': 88290, 'supblot': 88291, 'bailsman': 88292, \"turaquistan's\": 88293, 'leaden': 13419, 'primetime': 27615, 'understatement': 7691, 'amrapurkars': 82051, 'sheryll': 88295, '92nd': 51898, 'inflame': 88296, 'pilgrims': 48444, 'everone': 88297, 'dimeco': 88298, 'appalingly': 88299, 'delineation': 51899, \"2004's\": 40766, \"shining'\": 51900, 'director¡¦s': 88300, 'reproaches': 40767, 'thames': 23318, 'diamiter': 88301, 'moveis': 88302, 'mardi': 14352, \"wells'\": 11583, '850pm': 88303, 'sacrilage': 86742, 'redlitch': 40769, 'sadistically': 21894, 'lean': 6928, 'jacqualine': 88304, 'torin': 88305, 'disey': 88306, 'contributions': 11923, 'magnifying': 51901, 'cupboards': 51902, \"bedroom'\": 86759, 'showthread': 88307, 'someincredibly': 88308, 'whig': 40770, 'roshan': 21839, 'celie': 11520, \"'hot'\": 40771, \"susan's\": 18459, 'visualizing': 40772, 'hinduism': 25217, 'trasforms': 88311, 'fences': 18500, 'fencer': 34667, 'snipes': 7574, 'sniper': 6434, 'abode': 30544, \"'caprica'\": 88312, \"'moviefreak'\": 88313, 'lynch': 2216, 'bedrooms': 30545, '713': 30546, 'honost': 88314, 'slickest': 51443, 'shuriikens': 88315, 'wishes': 3082, 'spectaculars': 51904, 'harlot': 88316, 'harlow': 7179, \"o'd\": 88317, 'notes': 3625, 'underscores': 15491, 'leader': 2118, 'minimizing': 51905, 'outdo': 18501, 'underscored': 20589, 'comlex': 88318, 'noted': 3209, \"put's\": 57149, 'frost': 7741, 'procession': 30547, 'bodybuilding': 63906, \"y'know\": 34668, \"serat's\": 51907, 'rrw': 51908, 'manipulating': 12995, 'fermenting': 34669, 'cronicles': 88319, 'stth': 88320, 'palminteri': 25218, 'page2': 88321, \"foywonder's\": 88322, \"1934's\": 51471, 'slur': 40774, 'waiting': 1061, 'relocate': 27617, 'ammunition': 16809, \"o'laughlin\": 88323, 'nightstick': 88324, 'flavoured': 88325, \"bowden's\": 88326, 'handedly': 10938, 'highwater': 51910, \"busted's\": 51911, 'tarred': 51912, \"imposter's\": 88327, 'insouciance': 40775, 'cyclist': 51913, \"lestrade's\": 88328, 'metro': 13879, 'spiced': 18502, 'hulled': 55145, 'microman': 88329, 'parroting': 88330, 'bickers': 51914, \"'dog\": 51915, 'simpleton': 20590, \"'devil'\": 40776, 'breeds': 23370, \"'don\": 34670, 'transportive': 88331, 'appearently': 88332, 'apple': 7692, 'deputy': 6930, 'graça': 51916, 'dwarves': 51917, 'overbaked': 40778, 'offends': 16109, 'herzog': 16810, 'ghim': 88333, 'cataclysms': 88334, 'motor': 11035, 'coreys': 88335, 'apply': 6791, 'chandelere': 88336, 'rumiko': 51918, 'iced': 51919, 'baltimorean': 88337, 'bathroom': 3865, 'enging': 88338, 'reanimating': 88339, 'ices': 88340, 'metzger': 40779, 'weeping': 14353, 'december': 8287, \"sun's\": 30549, 'automobile': 15635, 'briers': 25219, 'epiphanal': 88342, 'porch': 18503, 'credulous': 40780, 'philanderer': 40781, \"'austin\": 88343, 'cooperate': 16811, \"ruby's\": 30550, \"'talky'\": 88344, 'avtar': 78708, 'faultless': 19489, 'echos': 36249, 'penis': 9090, 'annoy': 7807, \"ice'\": 40783, 'slaps': 12603, \"cgi'd\": 40784, 'foggiest': 88345, 'matelot': 88346, 'splinters': 40785, 'demonically': 88347, 'finacier': 88348, 'thepace': 88349, 'biscuit': 21241, 'proog': 30551, 'proof': 3167, 'tat': 21895, 'tau': 88350, 'arlen': 25220, 'tap': 3929, 'tar': 19490, 'yardsale': 88351, 'earnestness': 23371, 'matinee': 21896, 'dummee': 88352, 'tax': 6620, 'tay': 40786, 'taz': 51920, 'germanish': 88353, 'tad': 4415, 'kyer': 88354, 'tag': 4346, 'condescend': 88355, 'shipping': 30552, 'perplexities': 65038, 'tac': 40787, 'blaire': 62607, 'tam': 20591, 'tan': 17599, 'tao': 88356, 'onions': 51922, 'tai': 88357, 'taj': 34672, 'tak': 34673, 'twizzlers': 88358, \"'tinker\": 88359, 'inaugural': 51923, 'fortunate': 7575, 'japs': 30553, 'actualities': 51924, 'testators': 88360, 'japp': 25221, \"bury's\": 88361, 'taylors': 88362, 'panic': 4076, 'inflated': 18504, \"'drama\": 88363, 'perjury': 88364, 'sightseeing': 48760, 'footpath': 51925, 'skullduggery': 25222, \"blaine's\": 51926, 'fiancés': 88365, 'brushes': 15956, 'fortinbras': 51928, 'crawling': 11584, 'richthofen': 45479, 'elie': 88367, 'pah': 23314, 'elia': 13416, 'cumpsty': 88368, 'elio': 51929, 'unjustified': 25223, 'trophies': 88369, 'scoggins': 25224, \"'machinal'\": 88370, 'archiological': 88371, 'encyclicals': 88372, 'gunslinger': 14354, 'meir': 51930, 'reeve': 6625, 'voiceless': 88373, 'byw': 88374, 'terriers': 88375, 'byu': 40788, 'antonius': 34674, 'merchandise': 15071, 'bye': 5455, 'jukebox': 88376, 'violating': 19491, 'crash': 2484, 'yazaki': 88377, 'commended': 14895, 'knights': 8238, \"caliban's\": 88378, 'commender': 34675, \"melbourne's\": 88379, '183': 40790, 'crass': 9254, 'cauffiel': 51931, 'transmitter': 88380, 'easel': 88381, 'whiteclad': 88382, 'sinnister': 67103, 'standoffish': 51601, 'puya': 88383, 'tram': 88384, 'eased': 34560, 'fiddle': 18505, 'brontosaurus': 88385, \"by'\": 51932, 'tray': 30554, 'upheavals': 51933, 'cleverless': 84561, 'pitying': 19492, \"'dollman'\": 88386, 'bentley': 16111, 'pallance': 63915, 'ponderance': 88387, 'registration': 21903, 'bhand': 54048, 'apophis': 25226, 'fusing': 51934, 'bhang': 88388, 'celticism': 88389, 'achile': 88390, 'siana': 88391, \"breathnach's\": 88392, \"'masked\": 88393, 'pam': 9091, 'siani': 51935, 'phoebus': 40791, 'flapjack': 51936, \"'smallpox\": 88394, 'semen': 16219, \"'best\": 38663, \"montand's\": 51937, 'garcia': 9628, \"franko's\": 88395, 'outings': 12604, 'obstructions': 51938, 'dominion': 23173, \"daria's\": 51939, 'grits': 40792, 'slahsers': 88553, 'riddick': 66387, 'dictionary': 14355, 'nuit': 23373, \"ashram's\": 88396, 'abductor': 34679, 'displeasing': 88397, 'obliteration': 88398, 'bedazzled': 51941, 'kober': 19493, 'dormitories': 88399, 'cait': 88400, \"outing'\": 88401, 'segregated': 34680, 'substantiate': 88403, 'reexamined': 88404, 'horace': 25227, 'severing': 26024, 'grapevine': 70371, 'inseglet': 88406, 'caio': 51943, 'cain': 7914, 'californian': 31925, \"critique's\": 88407, \"munk's\": 87449, 'firehouse': 25229, \"wenders'\": 34681, '1888': 38486, 'jarrell': 40794, 'afgan': 88408, 'bislane': 29593, 'homere': 88409, 'treaters': 51944, 'hitherto': 27620, '185': 51945, 'olde': 30661, \"\\x91insignificance'\": 88410, 'harmlessness': 88411, 'vampress': 88412, 'homers': 88413, 'garard': 88414, 'destinations': 27621, 'flourish': 16813, 'myspace': 25232, 'accent': 1188, 'weberian': 88415, 'drood': 88416, 'drool': 14356, 'darma': 88417, 'thread': 5554, 'elbowroom': 88418, 'peoria': 40795, 'kiera': 16112, 'resurrects': 25233, 'greenlight': 20568, 'wanted': 470, 'constituent': 88419, 'evisceration': 40797, \"flik's\": 40798, 'osenniy': 88420, 'kiernan': 46193, 'nationalistic': 76077, 'swooned': 51948, 'tommy': 4041, 'morely': 88421, '\\x84bubble': 34682, 'leticia': 51949, \"'bruce\": 50313, 'harald': 40106, 'sculpture': 17626, 'bodice': 51950, 'beter': 88422, 'franfreako': 87566, 'outsmarted': 27206, 'clevage': 88423, 'wathced': 88424, 'woebegone': 40799, 'footwork': 34698, 'mongolian': 88425, 'pealed': 88426, 'donavon': 50183, 'florin': 88427, 'excusable': 27476, 'florid': 51952, 'allergies': 88428, 'oozing': 17627, 'agency': 5744, 'zhukov': 51953, 'maharashtrian': 88430, 'savagely': 14889, 'stying': 88431, \"den'\": 88432, 'divers': 18506, 'youuuu': 88433, 'crept': 22166, \"feud's\": 88434, 'centre': 5923, 'flesheaters': 88435, 'noggin': 27622, 'doyleluver': 88436, 'flawlessness': 88437, 'slitting': 40162, \"eustache's\": 30556, 'deny': 6107, \"mcelwee's\": 40801, \"quiroz's\": 88438, 'jpieczanski': 88439, 'thialnd': 88440, 'dens': 88441, 'syncretism': 88442, 'gingernuts': 88443, 'goodhearted': 51954, 'dent': 23374, 'cinématographe': 34684, 'xavier': 25234, 'dena': 88444, 'inane': 4482, 'discontinuous': 40802, 'mckellar': 88445, 'houswives': 88446, 'faded': 7859, 'myrna': 7798, 'illuminations': 31264, 'camion': 88447, 'upright': 15712, 'laughers': 88448, 'playbook': 88449, 'powerhouse': 12996, '500db': 88450, \"waterfall'\": 87708, 'foreclosure': 57743, 'hamburglar': 88451, 'asterix': 88452, \"generation'\": 51956, 'fans': 448, 'operational': 87720, 'candidates': 9455, 'lassiter': 51957, 'cordobes': 88453, 'thousands': 3083, 'forfeit': 88454, 'philipps': 20126, 'vacuousness': 88455, 'shlocky': 51958, 'workin': 88456, 'dope': 10719, \"skogland's\": 51959, '999': 16801, '998': 88457, 'woooooo': 88458, 'backdropped': 87756, 'argento': 13832, 'penetrate': 20593, 'wordy': 17628, 'ghetto': 6788, \"candidate'\": 88459, 'mulford': 88460, 'instructive': 40826, 'sappily': 88461, 'eccentrically': 87791, 'jansch': 88462, 'asian': 2185, 'generations': 5119, 'ebb': 30558, \"lifeforce'\": 88463, 'chrstmas': 88464, 'detatched': 88465, 'paulson': 30559, 'churchman': 87824, 'violet': 14357, 'gutters': 88467, 'scoured': 88468, 'etchings': 51962, \"kylie's\": 88469, 'closer': 2436, 'closes': 11036, 'panto': 88470, 'closet': 4250, 'magus': 88471, 'eggleston': 88472, 'genius': 1259, 'ashleigh': 34686, 'rumblings': 51963, 'panty': 40807, 'closed': 4587, 'dividends': 51964, 'simms': 27624, 'brosnan': 3974, 'linebacker': 51965, 'complement': 10552, \"baddies'\": 88473, 'addio': 88474, 'audiovisual': 51966, 'beverages': 21898, 'brigadoon': 17629, 'salesgirl': 40808, 'doesnot': 88475, 'sexploitation': 13882, \"jame's\": 88476, 'memento': 19494, 'barrett': 14358, 'soapbox': 30560, 'famke': 40809, 'profanity': 5398, 'b36s': 88477, 'windom': 88478, \"close'\": 88479, \"concert'\": 88480, 'clansmen': 34687, 'agito': 51968, 'wnbq': 88481, 'kaempfen': 88482, \"thaw's\": 27625, 'shoving': 14891, 'sleaziest': 34688, 'guerin': 88484, 'inflicted': 9876, 'neighborliness': 88485, 'withdrawn': 16113, 'withdrawl': 88486, 'freedmen': 88487, \"'balkanized'\": 88488, 'kenobi': 21899, 'bbe': 88489, 'thumbed': 88490, 'broadcasts': 17598, 'validating': 51969, \"boxers'\": 87959, 'jugars': 88491, 'agreements': 34689, 'impetuously': 88492, 'thumbes': 88493, 'bbs': 51971, 'bbq': 34690, 'butchers': 23375, 'verizon': 88494, 'butchery': 40810, \"genie's\": 88495, 'señor': 46439, 'saner': 88496, \"'prestige'\": 51972, 'revivalist': 48235, \"carface's\": 40811, \"'deepness'\": 88497, \"agreement'\": 88498, 'aiieeee': 88499, 'beckettian': 69861, 'maciste': 88500, 'hounsou': 88501, 'intresting': 80934, 'not\\x85repeat': 88502, 'nilamben': 88503, 'klass': 81345, 'funnist': 88504, 'marshmorton': 39736, 'hobbyhorse': 88022, \"marcel's\": 51973, 'restructure': 88505, 'madrasi': 88506, 'agentine': 88507, 'garda': 40812, \"biggie's\": 40813, 'schlöndorff': 76092, '15\\x96year': 88508, 'gassing': 40814, 'stadiums': 34693, 'vintage': 6626, 'loutishness': 51975, \"montana's\": 25206, 'nwhere': 88509, '\\x84batman': 88061, 'satrapi': 88066, 'panning': 20594, 'keneth': 40815, 'fomentation': 88510, 'hopton': 88511, \"sousa's\": 51977, 'tactful': 51978, 'booz': 88512, 'craigs': 88513, 'brumes': 88514, 'snatchers': 16814, 'appropriated': 30530, 'baruc': 88515, 'albin': 88516, \"'penis'\": 51980, 'antipasto': 88517, 'overwork': 88518, 'appropriates': 88519, 'siouxie': 88520, 'outgrow': 40816, 'breaker': 17630, 'boor': 51981, \"'bubbling'\": 88521, 'rocker': 9877, \"arcand's\": 88522, \"chappelle's\": 77256, 'wren': 88523, 'lowlights': 51982, 'storia': 88524, 'mighty': 4884, 'hmapk': 88525, \"archeologist's\": 88526, 'downfalls': 34694, 'shigeru': 35318, 'disturbance': 19496, 'serbs': 12605, 'ensued': 22958, 'sky': 1746, 'morbidly': 21900, 'googling': 51983, \"'intentional'\": 82114, 'skg': 88527, 'adoption': 16106, 'timecrimes': 88528, 'serbo': 88529, 'ski': 16114, 'knob': 30562, 'saturation': 25238, 'sick': 1192, 'refusals': 54834, 'protesters': 21901, 'kristofferson': 8774, 'siemens': 51985, 'deviances': 53304, 'know': 121, 'knot': 30563, 'variants': 88530, 'renyolds': 40817, 'iberia': 17631, \"annabelle's\": 88531, 'bedfellows': 40818, 'praarthana': 88532, 'losted': 88533, 'starrer': 34695, 'dances': 4200, \"one''willard'\": 88534, 'cutlet': 88237, 'eviction': 39794, 'isolytic': 88536, 'starred': 2681, 'traitors': 27626, 'nigga': 88259, 'shabby': 12257, 'waqt': 13883, \"matrix'\": 23376, 'junk': 2579, 'mosquitoes': 34696, 'libretto': 34697, 'kattan': 13418, 'beguine': 88537, 'soninha': 34040, 'sequency': 88538, 'dubbers': 88539, 'zillionaire': 88294, \"gettin'\": 51988, 'zanuck': 17632, 'leaded': 51989, 'strolls': 29326, 'moguls': 40820, 'catepillar': 88540, 'sette': 40821, 'murdoch': 20588, 'murdock': 23377, 'uneffective': 88541, 'poets': 16812, 'miniskirts': 88542, 'pensaba': 88543, 'thoroughfare': 88544, \"o'donoghugh\": 51991, 'arganauts': 76917, 'swinginest': 88545, 'consuming': 13884, 'wesley': 7915, 'daud': 88546, 'guffaws': 30564, 'empahh': 88547, 'nationalities': 30565, 'throne': 8594, 'intercontenital': 88548, 'throng': 51992, 'getting': 394, 'klineschloss': 51993, 'dependence': 21902, 'dependency': 25240, 'epiphany': 19497, 'fudd': 25241, 'fitzgibbon': 88549, 'bugle': 27627, 'unoticeable': 88550, 'couldnt': 24879, 'gunther': 30566, 'cultureless': 57756, \"'hall\": 88551, \"mariner's\": 88552, 'blabbing': 46559, 'dispossessed': 34678, 'deceptiveness': 76816, 'patronisingly': 51994, 'maize': 88554, 'uncontrollably': 21904, 'expeditious': 88555, 'effeminate': 10963, 'hypnotism': 27628, \"'half\": 88556, 'lederer': 88557, 'uncontrollable': 14359, 'mimbos': 48238, \"i've\": 204, 'proving': 5974, 'desando': 88429, 'baaaaad': 40824, 'gereco': 44477, \"bearings'\": 88558, 'kensit': 51997, 'wight': 88559, 'austreim': 75731, 'contradictory': 16791, 'bratwurst': 51998, \"may's\": 14128, 'contradictors': 88560, 'amitabhs': 88561, 'jaffa': 19498, 'jaffe': 11924, 'howdoilooknyc': 62008, \"olan's\": 88562, 'ornella': 51999, 'bitva': 57759, 'fountainhead': 88563, 'reble': 88564, 'percival': 88565, 'lubricated': 88566, \"matsumoto's\": 34700, 'heralding': 88567, 'hirschbiegel': 52001, \"baywatch'\": 88568, 'odilon': 88569, 'meaningless': 4042, 'gnawing': 40827, \"'solve'\": 88570, \"guard's\": 88571, \"yamada's\": 50090, 'spookfest': 57761, 'airsoft': 88573, 'abhay': 11306, 'spanky': 23378, 'urrrghhh': 88574, 'ev': 88575, 'chicatillo': 88576, 'transacting': 88577, \"'la\": 27630, 'percent': 8925, 'oprah': 7996, 'sics': 88578, 'illinois': 11925, 'dogtown': 40828, 'roars': 20595, 'branch': 9456, 'kerouac': 52002, 'wheelers': 88579, 'sica': 20596, 'lance': 6435, \"pipe's\": 88580, 'discretionary': 64179, 'contends': 40829, 'copywrite': 88581, 'geysers': 52003, 'artbox': 88582, 'cronyn': 52004, 'hardboiled': 52005, \"voorhees'\": 88583, '35mm': 16815, \"'l'\": 88584, 'paget': 18509, 'expands': 20597}\n"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "print(len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整体的序列向右移三位，如下添加四个自定义的字符\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"<START>thisfilmwasjustbrilliantcastinglocationscenerystorydirectioneveryone'sreallysuitedtheparttheyplayedandyoucouldjustimaginebeingthererobert<UNK>isanamazingactorandnowthesamebeingdirector<UNK>fathercamefromthesamescottishislandasmyselfsoilovedthefacttherewasarealconnectionwiththisfilmthewittyremarksthroughoutthefilmweregreatitwasjustbrilliantsomuchthatiboughtthefilmassoonasitwasreleasedfor<UNK>andwouldrecommendittoeveryonetowatchandtheflyfishingwasamazingreallycriedattheenditwassosadandyouknowwhattheysayifyoucryatafilmitmusthavebeengoodandthisdefinitelywasalso<UNK>tothetwolittleboy'sthatplayedthe<UNK>ofnormanandpaultheywerejustbrilliantchildrenareoftenleftoutofthe<UNK>listithinkbecausethestarsthatplaythemallgrownuparesuchabigprofileforthewholefilmbutthesechildrenareamazingandshouldbepraisedforwhattheyhavedonedon'tyouthinkthewholestorywassolovelybecauseitwastrueandwassomeone'slifeafterallthatwassharedwithusall\""
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding填充的字符\n",
    "word_index['<PAD>'] = 0 \n",
    "# 每个字符开始的字符\n",
    "word_index['<START>'] = 1\n",
    "# 不知道的字符设置\n",
    "word_index['<UNK>'] = 2\n",
    "# 每个字符结束的字符\n",
    "word_index['<END>'] = 3\n",
    "\n",
    "# 将word_index 反转成index：word\n",
    "reverse_word_index = {v:k for k, v in word_index.items()}\n",
    "\n",
    "decode_review = lambda test_ids:''.join([reverse_word_index.get(word_id, '<UNK>') for word_id in test_ids])\n",
    "\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n"
    }
   ],
   "source": [
    "max_length = 500\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_data, # list of list\n",
    "    value=word_index['<PAD>'],\n",
    "    padding='post', # post放在补全的后面，pre放在补全的前面\n",
    "    maxlen = max_length,\n",
    ")\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_data, # list of list\n",
    "    value=word_index['<PAD>'],\n",
    "    padding='post', # post放在补全的后面，pre放在补全的前面\n",
    "    maxlen = max_length,\n",
    ")\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 500, 16)           160000    \n_________________________________________________________________\nbidirectional (Bidirectional (None, 500, 128)          10368     \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 128)               24704     \n_________________________________________________________________\ndense (Dense)                (None, 64)                8256      \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 203,393\nTrainable params: 203,393\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# 每个embedding定为长度为16的向量\n",
    "max_length = 500\n",
    "embedding_dim = 16\n",
    "batch_size = 128\n",
    "# 实现一个单层单向的rnn\n",
    "bi_rnn_model = keras.models.Sequential([\n",
    "    # 1. define matrix : [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    # 添加一个rnn层，先定义一个简单的rnn网络层，return_sequences= False表示我们只是用最后的输出\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(units=64, return_sequences=\n",
    "    True)),\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(units=64, return_sequences=\n",
    "    False)),\n",
    "    # 全连接层\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "\n",
    "])\n",
    "bi_rnn_model.summary()\n",
    "bi_rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 20000 samples, validate on 5000 samples\nEpoch 1/30\n20000/20000 [==============================] - 215s 11ms/sample - loss: 0.7015 - accuracy: 0.5006 - val_loss: 0.6993 - val_accuracy: 0.5062\nEpoch 2/30\n20000/20000 [==============================] - 226s 11ms/sample - loss: 0.6974 - accuracy: 0.5046 - val_loss: 0.6942 - val_accuracy: 0.4936\nEpoch 3/30\n20000/20000 [==============================] - 236s 12ms/sample - loss: 0.6858 - accuracy: 0.5458 - val_loss: 0.6914 - val_accuracy: 0.5434\nEpoch 4/30\n20000/20000 [==============================] - 224s 11ms/sample - loss: 0.6369 - accuracy: 0.6403 - val_loss: 0.7082 - val_accuracy: 0.5534\nEpoch 5/30\n20000/20000 [==============================] - 212s 11ms/sample - loss: 0.5406 - accuracy: 0.7301 - val_loss: 0.7495 - val_accuracy: 0.5604\nEpoch 6/30\n20000/20000 [==============================] - 209s 10ms/sample - loss: 0.4265 - accuracy: 0.8092 - val_loss: 0.8857 - val_accuracy: 0.5482\nEpoch 7/30\n20000/20000 [==============================] - 233s 12ms/sample - loss: 0.3316 - accuracy: 0.8644 - val_loss: 0.9724 - val_accuracy: 0.5532\nEpoch 8/30\n20000/20000 [==============================] - 221s 11ms/sample - loss: 0.2516 - accuracy: 0.9030 - val_loss: 1.1269 - val_accuracy: 0.5552\nEpoch 9/30\n20000/20000 [==============================] - 208s 10ms/sample - loss: 0.1881 - accuracy: 0.9301 - val_loss: 1.3303 - val_accuracy: 0.5516\nEpoch 10/30\n20000/20000 [==============================] - 188s 9ms/sample - loss: 0.1525 - accuracy: 0.9456 - val_loss: 1.3350 - val_accuracy: 0.5602\nEpoch 11/30\n20000/20000 [==============================] - 180s 9ms/sample - loss: 0.1111 - accuracy: 0.9635 - val_loss: 1.4699 - val_accuracy: 0.5504\nEpoch 12/30\n20000/20000 [==============================] - 179s 9ms/sample - loss: 0.0778 - accuracy: 0.9749 - val_loss: 1.7898 - val_accuracy: 0.5594\nEpoch 13/30\n20000/20000 [==============================] - 183s 9ms/sample - loss: 0.0631 - accuracy: 0.9800 - val_loss: 1.7918 - val_accuracy: 0.5594\nEpoch 14/30\n20000/20000 [==============================] - 189s 9ms/sample - loss: 0.0605 - accuracy: 0.9801 - val_loss: 1.9252 - val_accuracy: 0.5664\nEpoch 15/30\n20000/20000 [==============================] - 201s 10ms/sample - loss: 0.0515 - accuracy: 0.9833 - val_loss: 2.0438 - val_accuracy: 0.5622\nEpoch 16/30\n20000/20000 [==============================] - 185s 9ms/sample - loss: 0.0353 - accuracy: 0.9900 - val_loss: 2.2430 - val_accuracy: 0.5592\nEpoch 17/30\n20000/20000 [==============================] - 190s 10ms/sample - loss: 0.0328 - accuracy: 0.9900 - val_loss: 2.0644 - val_accuracy: 0.5654\nEpoch 18/30\n20000/20000 [==============================] - 208s 10ms/sample - loss: 0.0300 - accuracy: 0.9906 - val_loss: 2.2806 - val_accuracy: 0.5642\nEpoch 19/30\n20000/20000 [==============================] - 185s 9ms/sample - loss: 0.0249 - accuracy: 0.9925 - val_loss: 2.4587 - val_accuracy: 0.5498\nEpoch 20/30\n20000/20000 [==============================] - 197s 10ms/sample - loss: 0.0235 - accuracy: 0.9922 - val_loss: 2.5167 - val_accuracy: 0.5550\nEpoch 21/30\n20000/20000 [==============================] - 219s 11ms/sample - loss: 0.0226 - accuracy: 0.9920 - val_loss: 2.4859 - val_accuracy: 0.5494\nEpoch 22/30\n20000/20000 [==============================] - 208s 10ms/sample - loss: 0.0215 - accuracy: 0.9929 - val_loss: 2.7222 - val_accuracy: 0.5498\nEpoch 23/30\n20000/20000 [==============================] - 203s 10ms/sample - loss: 0.0205 - accuracy: 0.9938 - val_loss: 2.4929 - val_accuracy: 0.5522\nEpoch 24/30\n18432/20000 [==========================>...] - ETA: 14s - loss: 0.0154 - accuracy: 0.9953"
    }
   ],
   "source": [
    "# 0.2作为验证集\n",
    "bi_rnn_history = bi_rnn_model.fit(train_data, train_labels, epochs=30, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data['val_'+label] = history.history['val_'+label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(bi_rnn_history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(bi_rnn_history, 'loss', 30, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_rnn_model.evaluate(test_data, test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像分析\n",
    "过拟合现象\n",
    "对valid数据loss比较小，但是对于预测数据loss过于大\n",
    "解决办法：\n",
    "过拟合解决方法：\n",
    "\n",
    "1. 增加训练数据，数据量变多\n",
    "2. L1、L2...regularization\n",
    "    - y=Wx\n",
    "    - l1 : cost = (Wx-realy)^2 + abs(W)\n",
    "    - l2 : cost = (Wx-realy)^2 + sqrt(W)\n",
    "    - l3 : cost = (Wx-realy)^2 + pow(W)\n",
    "3. 简化神经网络结构\n",
    "    - droput regularization\n",
    "\n",
    "跑一次训练集要好久，顶不住，方法mark了以后加入到自己的项目做分析。。"
   ]
  }
 ]
}